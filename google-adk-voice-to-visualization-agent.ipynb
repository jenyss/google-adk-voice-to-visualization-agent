{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986b0d3-02e2-443b-8d12-b3b951e75dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-adk\n",
    "%pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae55017-599c-4c7c-a907-7c658ba4b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas python-dotenv duckdb numpy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3c23f-7445-4bce-85eb-c77c8574fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4794f53-a0aa-4dca-aee0-fb5a8ade0f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key_openai=os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key_google=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if api_key_openai and api_key_google:\n",
    "    print(\"Keys loaded\")\n",
    "else:\n",
    "    print(\"Keys are not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eaeb196-1a4a-46ea-a9aa-7a3afa35d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"  # Replace with your actual key here OR add it to your .env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daf4d0c-df89-430b-8547-0c107a7ec5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.genai.types import GenerationConfig\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai.types import Content, Part\n",
    "from google.adk.tools import google_search\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "import asyncio\n",
    "\n",
    "# from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fee0551-f0d9-472e-ad09-33e6f584de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from typing import TypedDict, Optional, List, Dict, Union, Any\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e53b27-3982-404f-8b17-30397c4e14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    with open(file_path, 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model=\"whisper-1\",\n",
    "            # response_format=\"verbose_json\",\n",
    "            # timestamp_granularities=[\"word\"]  # Supports [word, segment] granularity\n",
    "        )\n",
    "\n",
    "    return transcript.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57aafbd3-e2c6-4ae7-a101-fc65cd807a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def refine_prompt(transcribed_text):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": \"\"\"You are an assistant that reformulates user queries into structured prompts for data analysis and visualization.\n",
    "                Always follow these steps:\n",
    "                1. Identify the key data points, dimensions, metrics, filters, or chart types mentioned in the user's query.\n",
    "                2. Reformulate the user's input into a clear, concise, and structured prompt that can be accurately interpreted by an LLM.\n",
    "                3. Ensure that all data points, relationships, and visualization intents from the original request are preserved in the final reformulated prompt.\n",
    "                4. If any part of the user‚Äôs request is ambiguous or vague, make reasonable assumptions but preserve intent.\n",
    "             \"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": transcribed_text}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad20fcf7-80c6-421b-a84e-410403ce43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Robust preprocessing of DataFrame to handle empty values.\"\"\"\n",
    "    # Convert empty strings and whitespace to None\n",
    "    df = df.replace(r'^\\s*$', None, regex=True)\n",
    "    # Convert NaN strings to None\n",
    "    df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "    # Convert pandas NaN to None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preview_excel_structure(input_str: str, tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Use this first to examine the Excel file structure and data types. \n",
    "    The input should be a JSON string with format: {\"file_name\": \"your_file.xlsx\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(input_str)\n",
    "        file_name = data.get(\"file_name\")\n",
    "        if not file_name:\n",
    "            return json.dumps({\"error\": \"File name must be provided\"})\n",
    "\n",
    "        df = pd.read_excel(file_name)\n",
    "        df = preprocess_dataframe(df)  # Apply preprocessing\n",
    "        df_sample = df.head(3).astype(str)\n",
    "\n",
    "        print(\"‚úÖ Preview successful\")\n",
    "        display(df_sample)\n",
    "\n",
    "        result = {\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
    "            \"sample_rows\": df_sample.to_dict(orient=\"records\")\n",
    "        }\n",
    "\n",
    "        # Persist result to state\n",
    "        tool_context.state[\"preview_structure\"] = result\n",
    "        tool_context.state[\"file_name\"] = file_name  # also store file name for later tools\n",
    "        tool_context.state[\"full_data_rows\"] = df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "        # print(\"üîç Tool Context State:\\n\", tool_context.state.to_dict())\n",
    "\n",
    "        return json.dumps({ \"result\": result })\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({ \"error\": str(e) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67b58f4-0a07-41db-bef2-76692689ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_duckdb_query(input_data: dict, tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool for SQL operations (GROUP BY, aggregations, etc.).\n",
    "\n",
    "    Args:\n",
    "        input_data (dict): A dictionary with the following keys:\n",
    "            - \"file_name\" (str): Name of the Excel file to query (optional if already stored in state).\n",
    "            - \"query\" (str): SQL query to run against the file.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string with structure:\n",
    "            {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"...\",\n",
    "                \"result\": {\n",
    "                    \"columns\": [...],\n",
    "                    \"rows\": [...]\n",
    "                }\n",
    "            }\n",
    "            or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Validate input\n",
    "        if not isinstance(input_data, dict):\n",
    "            return json.dumps({\"error\": \"Input must be a dictionary.\"})\n",
    "\n",
    "        query = input_data.get(\"query\")\n",
    "        \n",
    "        file_name = input_data.get(\"file_name\") or tool_context.state.get(\"file_name\")\n",
    "\n",
    "        if not file_name:\n",
    "            return json.dumps({\"error\": \"'file_name' must be provided or available in state.\"})\n",
    "\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            return json.dumps({\"error\": \"'query' must be a non-empty string.\"})\n",
    "\n",
    "        print(\"\\nüîç Executing DuckDB query:\")\n",
    "        print(query)\n",
    "        print(file_name)\n",
    "\n",
    "        # df = pd.read_excel(file_name)\n",
    "        df = pd.DataFrame(tool_context.state[\"full_data_rows\"])\n",
    "\n",
    "        df = preprocess_dataframe(df)\n",
    "\n",
    "        with duckdb.connect() as con:\n",
    "            con.register(\"data\", df)\n",
    "            query = query.replace(file_name, \"data\")\n",
    "            result = con.execute(query).fetchdf()\n",
    "            print(\"‚úÖ Query successful\")\n",
    "            display(result)\n",
    "\n",
    "            if isinstance(result, pd.DataFrame):\n",
    "                df_processed = result.copy()\n",
    "                df_processed = df_processed.replace([float('inf'), -float('inf')], None)\n",
    "                df_processed = df_processed.where(pd.notna(df_processed), None)\n",
    "\n",
    "                for column in df_processed.columns:\n",
    "                    if df_processed[column].dtype == 'object':\n",
    "                        df_processed[column] = df_processed[column].apply(\n",
    "                            lambda x: str(x) if x is not None else None\n",
    "                        )\n",
    "\n",
    "                result_dict = {\n",
    "                    \"columns\": df_processed.columns.tolist(),\n",
    "                    \"rows\": df_processed.to_dict(orient=\"records\")\n",
    "                }\n",
    "\n",
    "                # Store in state\n",
    "                tool_context.state[\"last_query\"] = query\n",
    "                tool_context.state[\"query_result\"] = result_dict\n",
    "\n",
    "                # print(\"üîç Tool Context State:\\n\", tool_context.state.to_dict())\n",
    "\n",
    "                return json.dumps({\n",
    "                    \"status\": \"success\",\n",
    "                    \"message\": \"Query executed successfully. You can now proceed to create the visualization.\",\n",
    "                    \"result\": result_dict\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                tool_context.state[\"last_query\"] = query\n",
    "                tool_context.state[\"query_result\"] = str(result)\n",
    "                return json.dumps({\"status\": \"success\", \"result\": str(result)})\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50a9fb00-3d3b-45e2-9b7b-1683ddf243dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization(input_data: dict, tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Create and immediately display various types of visualizations using Plotly.\n",
    "    Supports: \"line\", \"bar\", \"stacked_bar\", \"scatter\", \"box\", \"histogram\", \"pie\", \"heatmap\", \"sankey\"\n",
    "    \n",
    "    Args:\n",
    "        input_data (dict): A dictionary with the following keys:\n",
    "            {\n",
    "              \"data\": {\n",
    "                \"result\": {\n",
    "                  \"rows\": [...],\n",
    "                  \"columns\": [...]\n",
    "                }\n",
    "              },\n",
    "              \"plot_type\": \"...\",\n",
    "              \"x\": \"...\",\n",
    "              \"y\": \"...\",\n",
    "              \"title\": \"...\",\n",
    "              \"color\": \"...\",\n",
    "              \"source\": \"...\",    # For Sankey\n",
    "              \"target\": \"...\",    # For Sankey\n",
    "              \"value\": \"...\"      # For Sankey\n",
    "            }\n",
    "        tool_context (ToolContext): Injected by ADK to access session state.\n",
    "\n",
    "    Returns:\n",
    "        str: A success message or an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üìä create_visualization tool was called.\")\n",
    "        \n",
    "        # Fallback from context\n",
    "        if input_data is None and tool_context:\n",
    "            input_data = tool_context.state.get(\"query_result\")\n",
    "            print(\"üì• Used query_result from session state.\")\n",
    "\n",
    "        if not input_data:\n",
    "            return \"‚ùå Error: No input data provided for visualization.\"\n",
    "\n",
    "        print(\"üìä Input received for visualization:\", json.dumps(input_data, indent=2))\n",
    "        \n",
    "        params = input_data\n",
    "        data = params.get(\"data\")\n",
    "        plot_type = params.get(\"plot_type\", \"line\")\n",
    "        x = params.get(\"x\")\n",
    "        y = params.get(\"y\")\n",
    "        title = params.get(\"title\", \"\")\n",
    "        color = params.get(\"color\")\n",
    "        orientation = params.get(\"orientation\", \"v\")\n",
    "        barmode = params.get(\"barmode\", \"group\")\n",
    "        size = params.get(\"size\")\n",
    "        nbins = params.get(\"nbins\")\n",
    "        source = params.get(\"source\")\n",
    "        target = params.get(\"target\")\n",
    "        value = params.get(\"value\")\n",
    "\n",
    "        # Validate and convert\n",
    "        if not data or not x:\n",
    "            return \"‚ùå Error: Missing required parameters: data and x.\"\n",
    "        \n",
    "        if isinstance(data, dict) and \"result\" in data and \"rows\" in data[\"result\"]:\n",
    "            df = pd.DataFrame(data[\"result\"][\"rows\"])\n",
    "        else:\n",
    "            return \"‚ùå Error: Invalid data format.\"\n",
    "\n",
    "        # Layout\n",
    "        layout_settings = {\n",
    "            'title': {'text': title, 'x': 0.5, 'xanchor': 'center', 'font': dict(size=16)},\n",
    "            'plot_bgcolor': 'white',\n",
    "            'paper_bgcolor': 'white',\n",
    "            'font': dict(size=12),\n",
    "            'margin': dict(l=50, r=50, t=50, b=100),\n",
    "            'height': 800,\n",
    "            'width': 1100,\n",
    "            'template': 'plotly_white'\n",
    "        }\n",
    "\n",
    "        if plot_type == \"line\":\n",
    "            fig = px.line(df, x=x, y=y, color=color, title=title)\n",
    "            fig.update_traces(mode='lines+markers')\n",
    "        elif plot_type == \"bar\":\n",
    "            fig = px.bar(df, x=x, y=y, title=title, barmode=barmode, orientation=orientation)\n",
    "        elif plot_type == \"stacked_bar\":\n",
    "            df[color] = df[color].fillna(\"Unknown\").astype(str)\n",
    "            fig = px.bar(df, x=x, y=y, color=color, title=title, barmode=\"stack\", orientation=orientation)\n",
    "        elif plot_type == \"scatter\":\n",
    "            fig = px.scatter(df, x=x, y=y, color=color, size=size, title=title)\n",
    "        elif plot_type == \"box\":\n",
    "            fig = px.box(df, x=x, y=y, color=color, title=title)\n",
    "        elif plot_type == \"histogram\":\n",
    "            fig = px.histogram(df, x=x, color=color, nbins=nbins, title=title)\n",
    "        elif plot_type == \"pie\":\n",
    "            fig = px.pie(df, names=x, values=y if y else None, color=x, title=title)\n",
    "        elif plot_type == \"heatmap\":\n",
    "            if len(df.columns) < 3:\n",
    "                pivot_df = df.pivot(index=y, columns=x, values=color if color else 'value')\n",
    "                fig = px.imshow(pivot_df, title=title)\n",
    "            else:\n",
    "                fig = px.imshow(df, title=title)\n",
    "        elif plot_type == \"sankey\":\n",
    "            if not (source and target and value):\n",
    "                return \"‚ùå Error: Sankey diagrams require 'source', 'target', and 'value'.\"\n",
    "            \n",
    "            # Sankey: compute unique node labels and indices\n",
    "            unique_nodes = list(dict.fromkeys(df[source].tolist() + df[target].tolist()))\n",
    "            source_indices = df[source].apply(lambda x: unique_nodes.index(x))\n",
    "            target_indices = df[target].apply(lambda x: unique_nodes.index(x))\n",
    "\n",
    "            # Add total flow per node\n",
    "            label_with_values = []\n",
    "            for node in unique_nodes:\n",
    "                total_val = df[df[source] == node][value].sum() + df[df[target] == node][value].sum()\n",
    "                label_with_values.append(f\"{node}\\n({total_val:,.2f})\")\n",
    "\n",
    "            fig = go.Figure(data=[go.Sankey(\n",
    "                arrangement=\"freeform\",\n",
    "                domain=dict(x=[0, 1], y=[0.1, 0.90]),\n",
    "                node=dict(\n",
    "                    pad=70, thickness=30,\n",
    "                    line=dict(color=\"black\", width=0.9),\n",
    "                    label=label_with_values\n",
    "                ),\n",
    "                link=dict(\n",
    "                    source=source_indices,\n",
    "                    target=target_indices,\n",
    "                    value=df[value]\n",
    "                )\n",
    "            )])\n",
    "            fig.update_layout(\n",
    "                title_text=title,\n",
    "                font=dict(size=16, family=\"Arial, sans-serif\", color=\"black\"),\n",
    "                plot_bgcolor='white',\n",
    "                paper_bgcolor='white',\n",
    "                margin=dict(l=50, r=50, t=50, b=50)\n",
    "            )\n",
    "        else:\n",
    "            return f\"‚ùå Error: Unsupported plot type: {plot_type}.\"\n",
    "\n",
    "        fig.update_layout(layout_settings)\n",
    "\n",
    "        if plot_type not in ['pie', 'heatmap']:\n",
    "            fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray', title_text=x.replace('_', ' ').title())\n",
    "            if y:\n",
    "                fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray', title_text=y.replace('_', ' ').title())\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        if tool_context:\n",
    "            tool_context.state[\"last_visualization\"] = {\n",
    "                \"plot_type\": plot_type,\n",
    "                \"x\": x, \"y\": y, \"title\": title, \"color\": color\n",
    "            }\n",
    "\n",
    "        # print(\"üîç Tool Context State:\\n\", tool_context.state.to_dict())\n",
    "\n",
    "        return \"‚úÖ Visualization displayed successfully.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Visualization error: {str(e)}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "849783df-5dc8-4b03-b627-b99415980f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LiteLlm(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "root_agent = Agent(\n",
    "    name=\"data_agent\",\n",
    "    model=llm,\n",
    "    # model=\"gemini-2.0-flash-exp\", #gemini-2.0-flash-exp, gemini-2.5-pro-preview-03-25 gemini-2.5-pro-exp-03-25\n",
    "    description=(\n",
    "        \"Agent to answer data questions and create visualizations.\"\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"\"\"\n",
    "You will be given a task to perform. You must follow these exact steps in order:\n",
    "\n",
    "1. Always call the `preview_excel_structure` tool first to get the Excel columns. Match user input terms to Excel column names. You MUST print the matched values before continuing.\n",
    "2. Use the matched column names to build SQL query using the `complex_duckdb_query` tool. \n",
    "\n",
    "**IMPORTANT:** \n",
    "   - The table is registered as `data`. \n",
    "   - DO NOT use the file name or sheet name. \n",
    "   - Always query using `FROM data`.\n",
    "\n",
    "   \n",
    "3. Use the results of the SQL query to generate a Plotly visualization using the `create_visualization` tool. \n",
    "   - If user asks for a specific chart type (e.g., Sankey), use that.\n",
    "   - ALWAYS convert wide-format data (e.g., a single row of many metrics) into long-format JSON before calling create_visualization.\n",
    "\n",
    "**Visualization Output Format (CRITICAL):**\n",
    "\n",
    "{\n",
    "  \"data\": { \"result\": { \"columns\": [...], \"rows\": [...] } },\n",
    "  \"plot_type\": \"...\",\n",
    "  \"x\": \"column_name_for_x_axis\",\n",
    "  \"y\": \"column_name_for_y_axis\",\n",
    "  \"title\": \"...\",\n",
    "  \"color\": \"...\",\n",
    "}\n",
    "\n",
    "\n",
    "**Sankey example:**\n",
    " {\n",
    "  \"data\": {\n",
    "    \"result\": {\n",
    "      \"rows\": [\n",
    "        { \"source\": \"Company\", \"target\": \"Contacted by recruiter\", \"value\": 1 },\n",
    "        { \"source\": \"Contacted by recruiter\", \"target\": \"Recruiter interview\", \"value\": 16 },\n",
    "        { \"source\": \"Recruiter interview\", \"target\": \"1st round\", \"value\": 7 },\n",
    "        { \"source\": \"1st round\", \"target\": \"Challenge / Assignment\", \"value\": 4 },\n",
    "        { \"source\": \"Challenge / Assignment\", \"target\": \"2nd round\", \"value\": 4 },\n",
    "        { \"source\": \"2nd round\", \"target\": \"3rd round\", \"value\": 1 },\n",
    "        { \"source\": \"3rd round\", \"target\": \"Opening closed / put on hold\", \"value\": 1 },\n",
    "        { \"source\": null, \"target\": \"1st round\", \"value\": 6 },\n",
    "      ],\n",
    "      \"columns\": [\"source\", \"target\", \"value\"]\n",
    "    }\n",
    "  },\n",
    "  \"plot_type\": \"sankey\",\n",
    "  \"title\": \"Job Application Process Transitions\",\n",
    "  \"x\": \"source\",\n",
    "  \"source\": \"source\",\n",
    "  \"target\": \"target\",\n",
    "  \"value\": \"value\"\n",
    "}\n",
    "   \n",
    "4. Display the visualization.\n",
    "5. Evaluate if the task is completed. If the visualization was shown successfully OR if task completed, stop execution!\n",
    "\n",
    "        \"\"\"\n",
    "    ),\n",
    "    tools=[preview_excel_structure, complex_duckdb_query, create_visualization],\n",
    "    output_key=\"last_agent_response\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e9f0815-0644-47cf-b8b4-d82307dfaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the session and runner\n",
    "session_service = InMemorySessionService()\n",
    "app_name = \"viz_app\"\n",
    "user_id = \"jeny\"\n",
    "session_id = \"session_viz_001\"\n",
    "session = session_service.create_session(app_name=app_name, user_id=user_id, session_id=session_id)\n",
    "\n",
    "runner = Runner(agent=root_agent, app_name=app_name, session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2932d68a-1c96-4bf6-8664-109f438c98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Record your question\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_on_enter(filename='question.wav', fs=44100):\n",
    "    print(\"üéôÔ∏è Recording started... Press Enter to stop.\")\n",
    "\n",
    "    recording = []\n",
    "    is_recording = True\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(\"‚ö†Ô∏è\", status)\n",
    "        if is_recording:\n",
    "            recording.append(indata.copy())\n",
    "\n",
    "    # Start recording stream\n",
    "    stream = sd.InputStream(samplerate=fs, channels=1, callback=callback)\n",
    "    stream.start()\n",
    "\n",
    "    try:\n",
    "        input(\"‚èπÔ∏è  Press Enter to stop recording: \")\n",
    "    finally:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "\n",
    "    print(\"üì• Finalizing audio...\")\n",
    "\n",
    "    audio_data = np.concatenate(recording, axis=0)\n",
    "    write(filename, fs, audio_data)\n",
    "    print(f\"‚úÖ Recording saved as: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b951ed4-a950-4b22-aa0b-d6bc5042a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_on_enter(\"question.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91b182d0-5344-4f44-9cb6-867ed7a515e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transcribe the audio\n",
    "transcribed_text = transcribe_audio('question.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b3e793e-ddd3-47db-8449-3611e64e9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a bar chart visualization representing the forecasted values for each channel.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Refine the transcribed text into a structured prompt\n",
    "refined_prompt = refine_prompt(transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad2b20-1e7f-4b7f-bf01-625040c3ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual file instruction\n",
    "file_instruction = \"Use file: data_export.xlsx\\n\"\n",
    "\n",
    "# Combine both into a single prompt\n",
    "combined_prompt = file_instruction + refined_prompt\n",
    "\n",
    "# Create a single user message\n",
    "user_message = Content(\n",
    "    role=\"user\",\n",
    "    parts=[Part(text=combined_prompt)]\n",
    ")\n",
    "\n",
    "# Run and display the final response\n",
    "for event in runner.run(user_id=user_id, session_id=session.id, new_message=user_message):\n",
    "    if event.is_final_response():\n",
    "        if event.content and event.content.parts:\n",
    "            print(event.content.parts[0].text)\n",
    "        else:\n",
    "            print(\"No final text response was returned by the agent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504a2b9-bb39-4867-9c5e-db6c8e216b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_voice_to_sql_agent(file_name: str = \"data_export.xlsx\", audio_filename: str = \"question.wav\"):\n",
    "#     \"\"\"\n",
    "#     Full voice-to-SQL pipeline:\n",
    "#     1. Records user's spoken question until Enter is pressed.\n",
    "#     2. Transcribes the audio using Whisper API.\n",
    "#     3. Refines the transcription into a structured data prompt using GPT.\n",
    "#     4. Sends the refined prompt to the data agent with the file reference.\n",
    "\n",
    "#     Args:\n",
    "#         file_name (str): The Excel file to use in the data prompt.\n",
    "#         audio_filename (str): The name of the file to save the recorded audio.\n",
    "#     \"\"\"\n",
    "#     print(\"üîÑ Starting voice-to-SQL flow...\")\n",
    "    \n",
    "#     # Step 1: Record voice\n",
    "#     record_on_enter(audio_filename)\n",
    "\n",
    "#     # Step 2: Transcribe\n",
    "#     transcribed_text = transcribe_audio(audio_filename)\n",
    "#     print(\"üìù Transcription:\\n\", transcribed_text)\n",
    "\n",
    "#     # Step 3: Refine into a structured prompt\n",
    "#     refined_prompt = refine_prompt(transcribed_text)\n",
    "#     print(\"üìã Refined prompt:\\n\", refined_prompt)\n",
    "\n",
    "#     # Step 4: Combine with file instruction\n",
    "#     file_instruction = f\"Use file: {file_name}\\n\"\n",
    "#     combined_prompt = file_instruction + refined_prompt\n",
    "\n",
    "#     # Step 5: Wrap as user message and run\n",
    "#     user_message = Content(\n",
    "#         role=\"user\",\n",
    "#         parts=[Part(text=combined_prompt)]\n",
    "#     )\n",
    "\n",
    "#     print(\"üöÄ Running agent...\")\n",
    "#     for event in runner.run(user_id=user_id, session_id=session.id, new_message=user_message):\n",
    "#         if event.is_final_response():\n",
    "#             if event.content and event.content.parts:\n",
    "#                 print(\"üìä Agent response:\\n\", event.content.parts[0].text)\n",
    "#             else:\n",
    "#                 print(\"‚ö†Ô∏è No final text response was returned by the agent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17291e1f-757e-4381-b8ab-f5336f568895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_voice_to_sql_agent(\"data_export.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b786b4-f93b-40fa-8e85-9c34fc1992c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
